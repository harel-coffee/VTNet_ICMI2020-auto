{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifying Confusion with RNN\n",
    "\n",
    "This notebook holds our confusion classification pipeline for various RNN architectures. Experiments occur with no data augmentation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch import utils\n",
    "from torch import autograd\n",
    "from torchvision import transforms, datasets\n",
    "\n",
    "import pickle\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import shutil\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "import train\n",
    "import utils\n",
    "from networks import ConfusionLSTM\n",
    "from networks import ConfusionGRU\n",
    "from imblearn.over_sampling import SMOTE \n",
    "\n",
    "MANUAL_SEED = 1\n",
    "\n",
    "np.random.seed(MANUAL_SEED)\n",
    "random.seed(MANUAL_SEED)\n",
    "torch.manual_seed(MANUAL_SEED)\n",
    "\n",
    "DEVICE = torch.device(\"cuda:2\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "MAX_SEQUENCE_LENGTH = 150\n",
    "INPUT_SIZE = 14\n",
    "HIDDEN_SIZE = 256\n",
    "OUTPUT_SIZE = 2\n",
    "\n",
    "BATCH_SIZE = 256\n",
    "\n",
    "train.MAX_SEQUENCE_LENGTH = MAX_SEQUENCE_LENGTH\n",
    "train.INPUT_SIZE = INPUT_SIZE\n",
    "train.BATCH_SIZE = BATCH_SIZE\n",
    "\n",
    "utils.MAX_SEQUENCE_LENGTH = MAX_SEQUENCE_LENGTH\n",
    "utils.INPUT_SIZE = INPUT_SIZE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The following cell implements 10-fold CV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def cross_validate(model_type, \n",
    "                    folds,\n",
    "                    epochs,\n",
    "                    criterion_type,\n",
    "                    optimizer_type,\n",
    "                    confused_path,\n",
    "                    not_confused_path,\n",
    "                    print_every,\n",
    "                    plot_every,\n",
    "                    hidden_size,\n",
    "                    num_layers,\n",
    "                    down_sample_training=True,\n",
    "                    early_stopping_metric='val_loss',\n",
    "                    early_stopping_patience=10,\n",
    "                    max_rate_decreases=300,\n",
    "                    rate_decay_patience=5,\n",
    "                    initial_learning_rate=0.001,\n",
    "                    k_neighbors=11,\n",
    "                    verbose=False):\n",
    "    \"\"\"\n",
    "        Perform Cross Validation of the model using k-folds.\n",
    "        \n",
    "        Args:\n",
    "            model_type (string): the type of RNN to use. Must be 'lstm', 'gru', or 'rnn'\n",
    "            epochs (int): the max number of epochs to train the model for each fold\n",
    "            criterion_type (string): the name loss function to use for training. Currently must be 'NLLLoss'\n",
    "            optimizer_type (string): the name of learning algorithm to use for training. ex 'Adam'\n",
    "            confused_path (string): the path to the folder containing the confused data samples\n",
    "            not_confused_path (string): the path to the folder containing the not_confused data samples\n",
    "            print_every (int): the number of batches to train for before printing relevant stats\n",
    "            plot_every (int): the number of batches to train for before recording relevant stats, which\n",
    "                will be plotted after each fold\n",
    "            hidden_size (int): the number of hidden units for each layer of the RNN\n",
    "            num_layers (int): the number of hidden_unit sized layers of the RNN\n",
    "            down_sample_training (boolean): if True training set will be balanced by down sampling not_confused\n",
    "            early_stopping (boolean): if True, training will stop when early_stopping_patience epochs of \n",
    "                of training have passed without improvement in validation AUC ROC score\n",
    "            early_stopping_patience (int): number of epochs without improvement before stopping training\n",
    "            rate_dacay (boolean): if True, learning rate will decrease every rate_decay_patience epochs that\n",
    "                pass without improvement to validation set AUC ROC score.\n",
    "            rate_decay_patience (int): number of epochs without imporovement in AUC ROC that can pass before\n",
    "                reducing the learning rate by half.\n",
    "            initial_learning_rate (float): the first learning rate to be used by the optimizer\n",
    "            verbose (boolean): if True, function will print additional stats\n",
    "\n",
    "        Returns: (list,list,list,list,list,list)\n",
    "            cv_val_accs (list): list containing the validation accuracy for each fold\n",
    "            cv_val_sensis (list): list containing the validation sensitivity for each fold\n",
    "            cv_val_specifs (list): list containing the validation specificity for each fold\n",
    "            cv_test_accs (list): list containing the test accuracy for each fold\n",
    "            cv_test_sensis (list): list containing the test sensitivity for each fold\n",
    "            cv_test_specifs (list): list containing the test specificity for each fold\n",
    "        \n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    confused_file_names = os.listdir(confused_path)\n",
    "    not_confused_file_names = os.listdir(not_confused_path)\n",
    "    if '.DS_Store' in confused_file_names:\n",
    "        confused_file_names.remove('.DS_Store')\n",
    "    if '.DS_Store' in not_confused_file_names:\n",
    "        not_confused_file_names.remove('.DS_Store')\n",
    "    \n",
    "    #ensure same items appear in folds, for reproducibility:\n",
    "    infile = open('../grouped_10_fold_split_list_3.pickle', 'rb')\n",
    "    split = pickle.load(infile)\n",
    "    infile.close()\n",
    "    \n",
    "    train_confused_splits = split[0]\n",
    "    test_confused_splits = split[1]\n",
    "    train_not_confused_splits = split[2]\n",
    "    test_not_confused_splits = split[3]\n",
    "    \n",
    "    \n",
    "    cv_val_accs = []\n",
    "    cv_val_sensis = []\n",
    "    cv_val_specifs = []\n",
    "    cv_val_aucs = []\n",
    "    \n",
    "    cv_test_accs = []\n",
    "    cv_test_sensis = []\n",
    "    cv_test_specifs = []\n",
    "    cv_test_aucs = []\n",
    "    cv_test_combinds = []\n",
    "    for k in range(folds):\n",
    "        print(\"\\nFold \", k+1)\n",
    "        \n",
    "        # Get data item file names for this fold and downsample not_confused to balance training set\n",
    "\n",
    "        train_confused, \\\n",
    "        train_not_confused, \\\n",
    "        val_confused, \\\n",
    "        val_not_confused = \\\n",
    "        utils.get_train_val_split(train_confused_splits[k], train_not_confused_splits[k], percent_val_set=0.2)\n",
    "        \n",
    "        if down_sample_training:\n",
    "            train_not_confused = random.sample(train_not_confused, k=(len(train_confused)*3))\n",
    "        \n",
    "        test_confused = test_confused_splits[k]\n",
    "        test_not_confused = test_not_confused_splits[k]\n",
    "\n",
    "        print(\"Number of confused training items before SMOTE: \", len(train_confused))\n",
    "        print(\"Number of not_confused training items before SMOTE: \", len(train_not_confused))\n",
    "\n",
    "        y = np.zeros(shape=(len(train_confused+train_not_confused),))\n",
    "        X = np.zeros(shape=(len(train_confused+train_not_confused), MAX_SEQUENCE_LENGTH*INPUT_SIZE))\n",
    "        for i, item in enumerate(train_confused):\n",
    "            x = utils.pickle_loader2(confused_path+item)\n",
    "            x = np.reshape(x, MAX_SEQUENCE_LENGTH*INPUT_SIZE)\n",
    "            X[i] = x\n",
    "\n",
    "        for i, item in enumerate(train_not_confused):\n",
    "            x = utils.pickle_loader2(not_confused_path+item)\n",
    "            x = np.reshape(x, MAX_SEQUENCE_LENGTH*INPUT_SIZE)\n",
    "            X[i+len(train_confused)] = x\n",
    "            y[i+len(train_confused)] = 1\n",
    "            \n",
    "        #sm = SMOTE(random_state=MANUAL_SEED, sampling_strategy=.08165)\n",
    "        sm = SMOTE(random_state=MANUAL_SEED, k_neighbors=k_neighbors)\n",
    "        X_res, y_res = sm.fit_sample(X,y)\n",
    "\n",
    "        print(\"confused items in training set: \", len(y_res[y_res==0]))\n",
    "        print(\"not_confused items in training set: \", len(y_res[y_res==1]))\n",
    "        print(\"confused items in validation set: \", len(val_confused))\n",
    "        print(\"not_confused items in validation set: \", len(val_not_confused))\n",
    "        \n",
    "        if verbose:\n",
    "            print(\"\\nTest confused items:\\n\")\n",
    "            print(test_confused)\n",
    "\n",
    "        local_train_confused_path = '../dataset/augmented/train_smote_200/confused/'\n",
    "        local_val_confused_path = '../dataset/augmented/val_smote_200/confused/'\n",
    "        local_test_confused_path = '../dataset/augmented/test_smote_200/confused/'\n",
    "        local_train_not_confused_path = '../dataset/augmented/train_smote_200/not_confused/'\n",
    "        local_val_not_confused_path = '../dataset/augmented/val_smote_200/not_confused/'\n",
    "        local_test_not_confused_path = '../dataset/augmented/test_smote_200/not_confused/'\n",
    "\n",
    "        # Remove any old directories\n",
    "        if os.path.exists(local_train_confused_path):\n",
    "            shutil.rmtree(local_train_confused_path)\n",
    "        if os.path.exists(local_val_confused_path):\n",
    "            shutil.rmtree(local_val_confused_path)\n",
    "        if os.path.exists(local_test_confused_path):\n",
    "            shutil.rmtree(local_test_confused_path)\n",
    "            \n",
    "        if os.path.exists(local_train_not_confused_path):\n",
    "            shutil.rmtree(local_train_not_confused_path)\n",
    "        if os.path.exists(local_val_not_confused_path):\n",
    "            shutil.rmtree(local_val_not_confused_path)\n",
    "        if os.path.exists(local_test_not_confused_path):\n",
    "            shutil.rmtree(local_test_not_confused_path)\n",
    "            \n",
    "        # Make new temp directories\n",
    "        os.makedirs(local_train_confused_path)\n",
    "        os.makedirs(local_train_not_confused_path)\n",
    "        for i in range(len(X_res)):\n",
    "            if y_res[i] == 0:\n",
    "                item = pd.DataFrame(np.reshape(X_res[i], (MAX_SEQUENCE_LENGTH,INPUT_SIZE)))\n",
    "                item.to_pickle(local_train_confused_path+'smote_confused_item_'+str(i)+'.pkl')\n",
    "            else:\n",
    "                item = pd.DataFrame(np.reshape(X_res[i], (MAX_SEQUENCE_LENGTH,INPUT_SIZE)))\n",
    "                item.to_pickle(local_train_not_confused_path+'smote_not_confused_item_'+str(i)+'.pkl')\n",
    "        \n",
    "        os.makedirs(local_val_confused_path)\n",
    "        for i in val_confused:\n",
    "            shutil.copy(src=confused_path+i,dst=local_val_confused_path+i)\n",
    "        \n",
    "        os.makedirs(local_test_confused_path)\n",
    "        for i in test_confused:\n",
    "            shutil.copy(src=confused_path+i,dst=local_test_confused_path+i)\n",
    "        \n",
    "            \n",
    "        os.makedirs(local_val_not_confused_path)\n",
    "        for i in val_not_confused:\n",
    "            shutil.copy(src=not_confused_path+i,dst=local_val_not_confused_path+i)\n",
    "        \n",
    "        os.makedirs(local_test_not_confused_path)\n",
    "        for i in test_not_confused:\n",
    "            shutil.copy(src=not_confused_path+i,dst=local_test_not_confused_path+i)\n",
    "\n",
    "        # Prepare training and validation data\n",
    "        training_data = datasets.DatasetFolder('../dataset/augmented/train_smote_200/', \n",
    "                                               loader=utils.pickle_loader2, \n",
    "                                               extensions='.pkl')\n",
    "\n",
    "        validation_data = datasets.DatasetFolder('../dataset/augmented/val_smote_200/', \n",
    "                                                 loader=utils.pickle_loader2, \n",
    "                                                 extensions='.pkl')\n",
    "        \n",
    "        test_data = datasets.DatasetFolder('../dataset/augmented/test_smote_200/', \n",
    "                                                 loader=utils.pickle_loader2, \n",
    "                                                 extensions='.pkl')\n",
    "        \n",
    "        test_data_loader = torch.utils.data.DataLoader(test_data, \n",
    "                                                  batch_size=BATCH_SIZE,\n",
    "                                                  shuffle=False,\n",
    "                                                  num_workers=10 if DEVICE.type == 'cuda:2' else 5,\n",
    "                                                  pin_memory=True, drop_last=False)\n",
    "        print(\"Training data: \", training_data)\n",
    "        print(\"Validation data: \", validation_data)\n",
    "        print(\"Test data: \", test_data)\n",
    "        \n",
    "        torch.manual_seed(MANUAL_SEED)\n",
    "\n",
    "        if model_type == 'lstm':\n",
    "            model = ConfusionLSTM(input_size=INPUT_SIZE, hidden_size=hidden_size, \n",
    "                           output_size=OUTPUT_SIZE, batch_size=BATCH_SIZE, num_layers=num_layers)\n",
    "            if verbose:\n",
    "                print(model)\n",
    "        elif model_type == 'gru':\n",
    "            model = ConfusionGRU(input_size=INPUT_SIZE, hidden_size=hidden_size, \n",
    "                           output_size=OUTPUT_SIZE, batch_size=BATCH_SIZE, num_layers=num_layers)\n",
    "            if verbose:\n",
    "                print(model)\n",
    "\n",
    "        else:\n",
    "            model = ConfusionRNN(input_size=INPUT_SIZE, hidden_size=hidden_size, \n",
    "                           output_size=OUTPUT_SIZE, batch_size=BATCH_SIZE, num_layers=num_layers)\n",
    "            if verbose:\n",
    "                print(model)\n",
    "\n",
    "        model = model.to(DEVICE)\n",
    "        \n",
    "        #save fresh model to clear any old ones out\n",
    "        torch.save(model.state_dict(), './best_rnn_smote_200'+'_fold_'+str(k) +'.pt')\n",
    "\n",
    "        \n",
    "        # Train and evaluate for the k'th fold\n",
    "        model, \\\n",
    "        (training_accs, \\\n",
    "        validation_accs, \\\n",
    "        training_losses, \\\n",
    "        training_aucs, \\\n",
    "        validation_losses, \\\n",
    "        validation_recalls, \\\n",
    "        validation_specifs, \\\n",
    "        validation_aucs, \\\n",
    "        val_thresh) = train.train(model=model, \n",
    "                                epochs=epochs, \n",
    "                                criterion_type=criterion_type, \n",
    "                                optimizer_type=optimizer_type, \n",
    "                                training_data=training_data, \n",
    "                                val_data=validation_data, \n",
    "                                print_every=print_every,\n",
    "                                plot_every=plot_every,\n",
    "                                early_stopping=True,\n",
    "                                early_stopping_metric=early_stopping_metric,\n",
    "                                early_stopping_patience=early_stopping_patience,\n",
    "                                rate_decay_patience=rate_decay_patience,\n",
    "                                max_rate_decreases=300,\n",
    "                                initial_learning_rate=initial_learning_rate,\n",
    "                                model_name='best_rnn_smote_200_fold_'+str(k),\n",
    "                                verbose=True,\n",
    "                                return_thresh=True)\n",
    "        \n",
    "        utils.plot_metrics(training_accs, training_losses, training_aucs,\n",
    "                     validation_accs, validation_losses, validation_recalls, \n",
    "                     validation_specifs, validation_aucs)\n",
    "        \n",
    "        # store metrics for last epoch of the current fold of CV\n",
    "        cv_val_accs.append(validation_accs[-1])\n",
    "        cv_val_sensis.append(validation_recalls[-1])\n",
    "        cv_val_specifs.append(validation_specifs[-1])\n",
    "        cv_val_aucs.append(validation_aucs[-1])\n",
    "        \n",
    "        test_accuracy, \\\n",
    "        test_recall, \\\n",
    "        test_specificity, \\\n",
    "        test_auc, \\\n",
    "        test_loss = utils.check_metrics(model, test_data_loader)\n",
    "        combined = (test_recall + test_specificity ) / 2.0\n",
    "\n",
    "        cv_test_accs.append(test_accuracy)\n",
    "        cv_test_sensis.append(test_recall)\n",
    "        cv_test_specifs.append(test_specificity)\n",
    "        cv_test_aucs.append(test_auc)\n",
    "        cv_test_combinds.append(combined)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "    #clean up temp directories\n",
    "    shutil.rmtree(local_val_confused_path)\n",
    "    shutil.rmtree(local_val_not_confused_path)\n",
    "    shutil.rmtree(local_train_confused_path)\n",
    "    shutil.rmtree(local_train_not_confused_path)\n",
    "        \n",
    "    if verbose:\n",
    "        print(\"\\n%d-fold CV accuracy: %f\"% (folds, sum(cv_val_accs)/len(cv_val_accs)))\n",
    "        print(\"%d-fold CV sensitivity: %f \"% (folds, sum(cv_val_sensis)/len(cv_val_sensis)))\n",
    "        print(\"%d-fold CV specificity: %f \"% (folds, sum(cv_val_specifs)/len(cv_val_specifs)))\n",
    "        print(\"%d-fold CV AUC: %f \"% (folds, sum(cv_val_aucs)/len(cv_val_aucs)))\n",
    "        print(\"\\n%d-fold test accuracy: %f\"% (folds, sum(cv_test_accs)/len(cv_test_accs)))\n",
    "        print(\"%d-fold test sensitivity: %f \"% (folds, sum(cv_test_sensis)/len(cv_test_sensis)))\n",
    "        print(\"%d-fold test specificity: %f \"% (folds, sum(cv_test_specifs)/len(cv_test_specifs)))\n",
    "        print(\"%d-fold test combined: %f \"% (folds, sum(cv_test_combinds)/len(cv_test_combinds)))\n",
    "\n",
    "        print(\"%d-fold test AUC: %f \"% (folds, sum(cv_test_aucs)/len(cv_test_aucs)))    \n",
    "    return cv_test_sensis, cv_test_specifs, cv_test_combinds, cv_test_aucs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sens = []\n",
    "spec = []\n",
    "comb = []\n",
    "auc = []\n",
    "\n",
    "\n",
    "for i in range(10):\n",
    "\n",
    "    np.random.seed(MANUAL_SEED+i)\n",
    "    random.seed(MANUAL_SEED+i)\n",
    "    torch.manual_seed(MANUAL_SEED+i)\n",
    "\n",
    "    sens_list, spec_list, comb_list, auc_list = cross_validate(model_type='gru',\n",
    "                   folds=10,\n",
    "                   epochs=100,\n",
    "                   criterion_type='NLLLoss',\n",
    "                   optimizer_type='Adam',\n",
    "                   confused_path='../dataset/augmented/confused_highly_valid_new/',\n",
    "                   not_confused_path='../dataset/augmented/not_confused_highly_valid_new/',\n",
    "                   print_every=1,\n",
    "                   plot_every=1,\n",
    "                   hidden_size=HIDDEN_SIZE,\n",
    "                   num_layers=1,\n",
    "                   early_stopping_metric='val_auc',\n",
    "                   early_stopping_patience=30,\n",
    "                   rate_decay_patience=10,\n",
    "                   initial_learning_rate=0.003,\n",
    "                   verbose=True)\n",
    "    # add mean of each measure for 10-fold CV to list\n",
    "    sens.append(np.mean(sens_list))\n",
    "    spec.append(np.mean(spec_list))\n",
    "    comb.append(np.mean(comb_list))\n",
    "    auc.append(np.mean(auc_list))\n",
    "    \n",
    "print(\"sensitivities: \", sens)\n",
    "print(\"specificities: \", spec)\n",
    "print(\"combined: \", comb)\n",
    "print(\"auc: \", auc)\n",
    "\n",
    "print(\"average sensitivity: \", np.mean(sens))\n",
    "print(\"average specificity: \", np.mean(spec))\n",
    "print(\"average combined: \", np.mean(comb))\n",
    "print(\"average auc: \", np.mean(auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#output from above cell needed to be cleared (too long to save), so I saved it here:\n",
    "sensitivities:  [0.7731520562770562, 0.7601109307359308, 0.7341314935064934, 0.7123430735930735, 0.7725541125541125, 0.7586958874458875, 0.7608306277056277, 0.7702137445887447, 0.7300676406926406, 0.7728030303030302]\n",
    "specificities:  [0.784921678662956, 0.7906244110088444, 0.8175996158638646, 0.8202607975190039, 0.7508108806063986, 0.8089859522746448, 0.7913672840412677, 0.7983119960739928, 0.8116556030981371, 0.8340328734099112]\n",
    "combined:  [0.7790368674700061, 0.7753676708723876, 0.7758655546851791, 0.7663019355560388, 0.7616824965802556, 0.783840919860266, 0.7760989558734476, 0.7842628703313687, 0.7708616218953889, 0.8034179518564708]\n",
    "auc:  [0.8117319063151832, 0.7925897904826205, 0.8159519482062955, 0.8122335544460914, 0.795228013742219, 0.8223236337368904, 0.8124265814100273, 0.8216778763418311, 0.7996696354163962, 0.8207653776013885]\n",
    "average sensitivity:  0.7544902597402596\n",
    "average specificity:  0.800857109255902\n",
    "average combined:  0.7776736844980808\n",
    "average auc:  0.8104598317698943"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7731520562770562\n",
      "0.7601109307359308\n",
      "0.7341314935064934\n",
      "0.7123430735930735\n",
      "0.7725541125541125\n",
      "0.7586958874458875\n",
      "0.7608306277056277\n",
      "0.7702137445887447\n",
      "0.7300676406926406\n",
      "0.7728030303030302\n"
     ]
    }
   ],
   "source": [
    "sensitivities = [0.7731520562770562, 0.7601109307359308, 0.7341314935064934, 0.7123430735930735, 0.7725541125541125, 0.7586958874458875, 0.7608306277056277, 0.7702137445887447, 0.7300676406926406, 0.7728030303030302]\n",
    "specificities = [0.784921678662956, 0.7906244110088444, 0.8175996158638646, 0.8202607975190039, 0.7508108806063986, 0.8089859522746448, 0.7913672840412677, 0.7983119960739928, 0.8116556030981371, 0.8340328734099112]\n",
    "for i in sensitivities:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.784921678662956\n",
      "0.7906244110088444\n",
      "0.8175996158638646\n",
      "0.8202607975190039\n",
      "0.7508108806063986\n",
      "0.8089859522746448\n",
      "0.7913672840412677\n",
      "0.7983119960739928\n",
      "0.8116556030981371\n",
      "0.8340328734099112\n"
     ]
    }
   ],
   "source": [
    "for i in specificities:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7790368674700061\n",
      "0.7753676708723876\n",
      "0.7758655546851791\n",
      "0.7663019355560388\n",
      "0.7616824965802556\n",
      "0.783840919860266\n",
      "0.7760989558734476\n",
      "0.7842628703313687\n",
      "0.7708616218953889\n",
      "0.8034179518564708\n"
     ]
    }
   ],
   "source": [
    "combined = [0.7790368674700061, 0.7753676708723876, 0.7758655546851791, 0.7663019355560388, 0.7616824965802556, 0.783840919860266, 0.7760989558734476, 0.7842628703313687, 0.7708616218953889, 0.8034179518564708]\n",
    "for i in combined:\n",
    "    print(i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
